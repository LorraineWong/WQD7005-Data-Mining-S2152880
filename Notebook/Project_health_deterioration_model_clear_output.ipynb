{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Ljp7JaFCAvhpyPQkolrWn2ZkvWI_9OeA",
      "authorship_tag": "ABX9TyPVtpBdx7BZinspqLgPyUQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorraineWong/WQD7005-Data-Mining-S2152880/blob/main/Notebook/Project_health_deterioration_model_clear_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ› ï¸ 0. Setup and Configuration**"
      ],
      "metadata": {
        "id": "_e3ul-5h2FDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Core Dependencies**\n",
        "\n",
        "Install all required Python packages for AI-powered feature engineering, modeling, and NLP tasks."
      ],
      "metadata": {
        "id": "W4odbMzjN5mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Core Dependencies\n",
        "!pip install -q numpy pandas matplotlib seaborn scikit-learn xgboost transformers imbalanced-learn tqdm"
      ],
      "metadata": {
        "id": "2kugXK56Ivlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Working Directory**\n",
        "\n",
        "All generated data files (e.g., raw outputs, datasets, summaries) will be saved to this path for clarity and version control."
      ],
      "metadata": {
        "id": "W-bg4zhK2yG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Working Directory in Colab/Drive\n",
        "import os\n",
        "my_file_path = \"/content/drive/MyDrive/UM Data Science Course Information/WQD7005/Assignment Project/\"\n",
        "os.makedirs(my_file_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "_6HOZczn23Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Authenticate Hugging Face**\n",
        "\n",
        "plan to access transformer models (e.g., MiniLM, BERT-tiny), login to Hugging Face Hub is required."
      ],
      "metadata": {
        "id": "Y0u3blm_24dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Hugging Face Token\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "qOIKptvK5gEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Securely Load Azure API Credentials**\n",
        "\n",
        "Using secrets.json avoids hardcoding sensitive information. This supports secure API usage and easier sharing of my notebook."
      ],
      "metadata": {
        "id": "zPJsGReO3GeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely Load Azure API Credentials\n",
        "# Azure endpoint and keys\n",
        "import json\n",
        "\n",
        "# Load secrets.json after upload\n",
        "secrets_file = os.path.join(my_file_path, \"secrets.json\")\n",
        "if os.path.exists(secrets_file):\n",
        "    with open(secrets_file, \"r\") as f:\n",
        "        secrets = json.load(f)\n",
        "\n",
        "endpoint = secrets[\"AZURE_ENDPOINT\"]\n",
        "subscription_key = secrets[\"AZURE_KEY\"]"
      ],
      "metadata": {
        "id": "HpsM2sGkK9qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Configure Azure OpenAI Client and Define GPT Prompt Wrapper**\n",
        "\n",
        "This step sets up the Azure OpenAI client and defines a reusable function for sending prompts to GPT-4o, enabling automated clinical text generation and interpretation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euL8-aeG4I0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Supporting Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from openai import AzureOpenAI\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Configure Azure OpenAI Client\n",
        "api_version = \"2024-12-01-preview\"\n",
        "deployment = \"gpt-4o\"\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "# Prompt execution wrapper for reuse\n",
        "def model_prompt(prompt, system_prompt=\"Act as a professional clinicians.\", temperature=0.7, max_tokens=4096):\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "LFEgr22edvyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Single Sample Data Generation via GPT (Validation Prompt)**\n",
        "\n",
        "To verify the response structure of GPT-4o by generating a realistic single-patient daily monitoring record, ensuring the output conforms to expected JSON schema for later batch generation."
      ],
      "metadata": {
        "id": "tlJZrmccPUQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Sample Data Generation via model\n",
        "data_prompt = \"\"\"\n",
        "Generate a single, realistic patient monitoring record for one randomly selected adult patient.\n",
        "\n",
        "Provide the following fields:\n",
        "- oxygen_saturation (in %)\n",
        "- heart_rate (in bpm)\n",
        "- temperature (in Â°C)\n",
        "- blood_pressure (systolic/diastolic, e.g. \"120/80\")\n",
        "- weight (in kg)\n",
        "- blood_glucose (in mg/dL)\n",
        "\n",
        "At the end, include a brief clinical_note (1â€“2 sentences, max 30 words) summarizing the patient status based on the values above. Use professional clinical tone with realistic variation (e.g. stable, recovering, mild concerns).\n",
        "\n",
        "Output as a valid JSON object with keys:\n",
        "oxygen_saturation, heart_rate, temperature, blood_pressure, weight, blood_glucose, clinical_note.\n",
        "\n",
        "Constraints:\n",
        "- Only output one JSON object.\n",
        "- No markdown or explanation.\n",
        "- Include realistic variation across different health conditions (e.g. fatigue, post-op, dietary changes, stress).\n",
        "- Ensure all fields are complete, no missing values.\n",
        "\"\"\"\n",
        "\n",
        "print(model_prompt(data_prompt))"
      ],
      "metadata": {
        "id": "zSjfEl3ZOf-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§¬ 1. Dataset Simulation and Feature Engineering**"
      ],
      "metadata": {
        "id": "Iz5nKvzqaU06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a synthetic patient dataset with labeled **note_status** was already generated and preprocessed in the previous assignment, so this section focuses on loading the prepared dataset, checking basic data structure, verifying label quality, and ensuring readiness for AI-driven feature engineering and modeling."
      ],
      "metadata": {
        "id": "mFIaDVEkhdu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load Preprocessed Dataset**\n",
        "\n",
        "Load the previously prepared patient dataset containing all required features and labels."
      ],
      "metadata": {
        "id": "D9fYuZfKhv0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load preprocessed dataset from previous assignment\n",
        "df = pd.read_csv(my_file_path + \"preprocessing_generate_patient_dataset.csv\")\n",
        "\n",
        "# Display basic info and preview\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "cs67snDQh4BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "Jm7Vn5Z4jwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "id": "nzGmOabDjx_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Data Structure and Integrity Check**\n",
        "\n",
        "Check data types, confirm absence of missing values, and review main variables."
      ],
      "metadata": {
        "id": "_9CTnQwviRSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data info and missing values\n",
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "EJGs8MQPiD8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Check Label Distribution**\n",
        "\n",
        "Review the distribution of the clinical status label (note_status) to ensure it is suitable for modeling."
      ],
      "metadata": {
        "id": "evPy6zPpicjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.countplot(\n",
        "    data=df,\n",
        "    x=\"note_status\",\n",
        "    order=df[\"note_status\"].value_counts().index,\n",
        "    palette=\"pastel\"\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Clinical Note Status Labels\", fontsize=14)\n",
        "plt.xlabel(\"Note Status\", fontsize=12)\n",
        "plt.ylabel(\"Number of Records\", fontsize=12)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add count labels on each bar\n",
        "for p in ax.patches:\n",
        "    count = int(p.get_height())\n",
        "    ax.annotate(f\"{count}\", (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha=\"center\", va=\"bottom\", fontsize=11, color=\"black\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mYYkOqXiZbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Standardize Note Status Label Encoding**\n",
        "\n",
        "Map the clinical status label (note_status) to standardized integer codes: 0 for Stable, 1 for Recovering, 2 for Deteriorating, and 3 for Critical. Replace the existing note_status_encoded with these values for consistency in downstream modeling."
      ],
      "metadata": {
        "id": "0CI9t4ljqfe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define standardized label mapping\n",
        "note_status_mapping = {\n",
        "    \"Stable\": 0,\n",
        "    \"Recovering\": 1,\n",
        "    \"Deteriorating\": 2,\n",
        "    \"Critical\": 3\n",
        "}\n",
        "\n",
        "# Apply mapping to the note_status column and overwrite note_status_encoded\n",
        "df['note_status_encoded'] = df['note_status'].map(note_status_mapping)\n",
        "\n",
        "# Preview mapping results\n",
        "print(df[['note_status', 'note_status_encoded']].head())\n",
        "print(df['note_status_encoded'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "e7AuceqKpyOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "The preprocessed patient dataset has been successfully loaded and validated. All core variables, including vital sign z-scores, clinical notes, and coded clinical status labels, are present with no missing values. The distribution of the â€œnote_statusâ€ label has been visualized, revealing the class imbalance issue that will be addressed in the modeling phase. After confirming the dataset structure and quality, we are ready for AI-driven NLP feature engineering and predictive modeling."
      ],
      "metadata": {
        "id": "VGT1k4s3lkk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ¤– 2. NLP Feature Engineering**"
      ],
      "metadata": {
        "id": "TBP0t4T1beJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Sentiment Analysis on Clinical Notes**\n",
        "\n",
        "Extract sentiment label and score from each clinical note using a transformer-based sentiment analysis model."
      ],
      "metadata": {
        "id": "L31gbQwWmDxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def get_sentiment(text):\n",
        "    result = sentiment_pipe(text[:512])[0]\n",
        "    return pd.Series([result['label'], result['score']])\n",
        "\n",
        "tqdm.pandas(desc=\"Sentiment Analysis\")\n",
        "df[['sentiment_label', 'sentiment_score']] = df['clinical_note'].progress_apply(get_sentiment)"
      ],
      "metadata": {
        "id": "oEuQcuP1mDEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview sentiment features\n",
        "print(df[['sentiment_label', 'sentiment_score']].head())"
      ],
      "metadata": {
        "id": "zCoeLv8Zpcqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.countplot(\n",
        "    data=df,\n",
        "    x=\"sentiment_label\",\n",
        "    order=df[\"sentiment_label\"].value_counts().index,\n",
        "    palette=\"pastel\"\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Sentiment Labels\", fontsize=14)\n",
        "plt.xlabel(\"Sentiment Label\", fontsize=12)\n",
        "plt.ylabel(\"Number of Records\", fontsize=12)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add count labels on each bar\n",
        "for p in ax.patches:\n",
        "    count = int(p.get_height())\n",
        "    ax.annotate(f\"{count}\", (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha=\"center\", va=\"bottom\", fontsize=11, color=\"black\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aAuxVjf0xuGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check for ambiguous clinical notes, I analyze the distribution of sentiment scores. Scores near 0.5 suggest uncertainty, while scores closer to 0 or 1 indicate confident classification."
      ],
      "metadata": {
        "id": "hIjvS5Bcv9K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of potential \"neutral\" cases (sentiment_score between 0.4 and 0.6)\n",
        "neutral_count = ((df['sentiment_score'] >= 0.4) & (df['sentiment_score'] <= 0.6)).sum()\n",
        "total = len(df)\n",
        "percent_neutral = 100 * neutral_count / total\n",
        "\n",
        "print(f\"Potential 'neutral' cases (score between 0.4 and 0.6): {neutral_count} ({percent_neutral:.2f}%)\")"
      ],
      "metadata": {
        "id": "0mRyIfK8vID7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of sentiment scores for all clinical notes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(df['sentiment_score'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Sentiment Scores\")\n",
        "plt.xlabel(\"Sentiment Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2n8cQfpgvZGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview random samples of \"neutral\" sentiment cases\n",
        "neutral_samples = df[(df['sentiment_score'] >= 0.4) & (df['sentiment_score'] <= 0.6)].sample(5)\n",
        "print(neutral_samples[['clinical_note', 'note_status','sentiment_label', 'sentiment_score']])"
      ],
      "metadata": {
        "id": "LGWhXBbewcmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment scores are strongly polarized, with very few notes falling in the ambiguous range (0.4â€“0.6). Only 2.8% of clinical notes show unclear sentiment, confirming that binary sentiment labels are appropriate for this dataset."
      ],
      "metadata": {
        "id": "Jk61QU8Jww8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Encode Sentiment Labels**\n",
        "\n",
        "Convert the sentiment labels into binary numeric values for downstream modeling. Encode sentiment labels as binary values (1=POSITIVE, 0=NEGATIVE) for model input."
      ],
      "metadata": {
        "id": "aAQUdNQ2xD3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentiment label as binary (POSITIVE=1, NEGATIVE=0)\n",
        "df['sentiment_label_encoded'] = df['sentiment_label'].map({'POSITIVE': 1, 'NEGATIVE': 0})\n",
        "\n",
        "# Preview encoded results\n",
        "print(df[['sentiment_label', 'sentiment_label_encoded']].head())\n",
        "print(df['sentiment_label_encoded'].value_counts())"
      ],
      "metadata": {
        "id": "JpMcJS-tyMHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Generate MiniLM Embeddings for Clinical Notes**\n",
        "\n",
        "Convert each clinical note into a dense vector using the MiniLM model, creating numerical features that capture the semantic meaning of the text."
      ],
      "metadata": {
        "id": "gp-OFEVPzLtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load MiniLM model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to get mean-pooled sentence embedding\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        emb = model(**inputs).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "# Generate embeddings with progress bar\n",
        "embeddings = []\n",
        "for note in tqdm(df['clinical_note'], desc=\"Generating MiniLM Embeddings\"):\n",
        "    embeddings.append(get_embedding(note))\n",
        "embeddings = np.vstack(embeddings)"
      ],
      "metadata": {
        "id": "eVQFyKX5zabG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame and merge with main df\n",
        "embeddings_df = pd.DataFrame(embeddings, columns=[f'embedding_{i+1}' for i in range(embeddings.shape[1])])\n",
        "df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "\n",
        "# Preview embedding features\n",
        "print(embeddings_df.shape)\n",
        "print(embeddings_df.head())"
      ],
      "metadata": {
        "id": "VJoz_K60zh6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ·ï¸ 3. Feature and Target Assignment**"
      ],
      "metadata": {
        "id": "xIRCU9gZbkvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Select Features for Modeling**\n",
        "\n",
        "Combine structured vital signs, sentiment analysis features, and MiniLM embeddings to form the initial input feature set."
      ],
      "metadata": {
        "id": "NxS2tCBjXmrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of structured vital sign features\n",
        "vital_features = [\n",
        "    'temperature_zscore', 'heart_rate_zscore', 'blood_glucose_zscore',\n",
        "    'oxygen_saturation_zscore', 'systolic_bp_zscore', 'diastolic_bp_zscore', 'weight_zscore'\n",
        "]\n",
        "\n",
        "# NLP features (sentiment + embeddings)\n",
        "nlp_features = ['sentiment_label_encoded', 'sentiment_score'] + [f'embedding_{i+1}' for i in range(embeddings_df.shape[1])]\n",
        "\n",
        "# Combine all features for model input\n",
        "feature_cols = vital_features + nlp_features\n",
        "X = df[feature_cols]\n",
        "print(\"Feature matrix shape:\", X.shape)"
      ],
      "metadata": {
        "id": "3Ft7SaiTzHQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Target Variable**\n",
        "\n",
        "Set the encoded clinical status label as the prediction target for multi-class classification."
      ],
      "metadata": {
        "id": "HhbiTTsb9uF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable (multi-class clinical status)\n",
        "y = df['note_status_encoded']\n",
        "print(\"Target distribution:\\n\", y.value_counts().sort_index())"
      ],
      "metadata": {
        "id": "JPvY9dgA91pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ”€ 4. Train-Test Split**\n"
      ],
      "metadata": {
        "id": "3b5S-8urbs96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Split the Dataset**\n",
        "\n",
        "Split the dataset into training and test sets, stratifying by the target variable to maintain class distribution."
      ],
      "metadata": {
        "id": "jztxeEiB-kaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80% for training, 20% for testing, stratified by class\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "print(\"Training target distribution:\\n\", y_train.value_counts().sort_index())\n",
        "print(\"Test target distribution:\\n\", y_test.value_counts().sort_index())"
      ],
      "metadata": {
        "id": "i1WPtABf-xHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **âš–ï¸ 5. Class Imbalance Handling (SMOTE)**"
      ],
      "metadata": {
        "id": "YxQYo2nZMzc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Balance the Training Set with SMOTE**\n",
        "\n",
        "Apply SMOTE to the training set to generate synthetic minority samples and balance class distribution. The process may take some time for high-dimensional data."
      ],
      "metadata": {
        "id": "X0xlj0bJ_rfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optional: label mapping for pretty output\n",
        "note_status_mapping = {0: \"Stable\", 1: \"Recovering\", 2: \"Deteriorating\", 3: \"Critical\"}\n",
        "\n",
        "# Print original class distribution (with labels)\n",
        "print(\"Original training class distribution:\")\n",
        "for k, v in Counter(y_train).items():\n",
        "    print(f\"  {note_status_mapping[k]} ({k}): {v}\")\n",
        "\n",
        "# Start timer\n",
        "start = time.time()\n",
        "\n",
        "# Run SMOTE with overall progress feel\n",
        "print(\"Applying SMOTE to balance classes...\")\n",
        "for _ in tqdm(range(1), desc=\"SMOTE Oversampling\"):\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end = time.time()\n",
        "print(f\"SMOTE completed in {end-start:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "sTN3HHJO_r_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print balanced class distribution (with labels)\n",
        "print(\"Balanced training class distribution:\")\n",
        "for k, v in Counter(y_train_bal).items():\n",
        "    print(f\"  {note_status_mapping[k]} ({k}): {v}\")\n",
        "\n",
        "print(\"Balanced training set shape:\", X_train_bal.shape)"
      ],
      "metadata": {
        "id": "3ETBqn6TCnOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸš€ 6. Model Development and Evaluation**"
      ],
      "metadata": {
        "id": "q3ucc5RZM79z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Step 1: Define the Traditional Model Evaluation Function**\n",
        "\n",
        "Define a general-purpose evaluation function for traditional models, reporting metrics, confusion matrix, and classification report."
      ],
      "metadata": {
        "id": "AQe4mtxPFpQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluate a classification model with standard metrics and visualize the confusion matrix.\n",
        "    Returns a summary dictionary for results table, including raw evaluation artifacts for LLM analysis.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import (\n",
        "        accuracy_score, f1_score, precision_score, recall_score,\n",
        "        classification_report, confusion_matrix\n",
        "    )\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
        "    prec_macro = precision_score(y_test, y_pred, average=\"macro\")\n",
        "    recall_macro = recall_score(y_test, y_pred, average=\"macro\")\n",
        "    f1_train = f1_score(y_train, y_train_pred, average=\"macro\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
        "    print(f\"Accuracy: {acc:.4f} | Macro F1: {f1_macro:.4f} | Precision: {prec_macro:.4f} | Recall: {recall_macro:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "    # Add these for LLM-friendly result summaries\n",
        "    cm_list = cm.tolist()  # For JSON/prompt/LLM\n",
        "    report_dict = classification_report(y_test, y_pred, digits=3, output_dict=True)  # For AI parsing\n",
        "    report_str = classification_report(y_test, y_pred, digits=3)  # For human reading\n",
        "\n",
        "    # Return summary dict\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Train F1\": round(f1_train, 4),\n",
        "        \"Test F1\": round(f1_macro, 4),\n",
        "        \"Accuracy\": round(acc, 4),\n",
        "        \"Precision\": round(prec_macro, 4),\n",
        "        \"Recall\": round(recall_macro, 4),\n",
        "        \"Confusion Matrix\": cm_list,\n",
        "        \"Classification Report (dict)\": report_dict,\n",
        "        \"Classification Report (str)\": report_str\n",
        "    }"
      ],
      "metadata": {
        "id": "UsNi_M8jC2Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Train and Evaluate Random Forest**\n",
        "\n",
        "Train a Random Forest classifier. Evaluate its test set performance using key metrics and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "PI5nmjRGC1f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "results_list = []\n",
        "\n",
        "# Train Random Forest on balanced training set\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=10,\n",
        "    min_samples_split=8,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "rf_results = evaluate_model(rf, X_train_bal, y_train_bal, X_test, y_test, model_name=\"Random Forest\")\n",
        "results_list.append(rf_results)"
      ],
      "metadata": {
        "id": "NJzH6motJMZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Train and Evaluate XGBoost**\n",
        "\n",
        "Train an XGBoost classifier using the same features and balanced data. Evaluate its performance using the same metrics for fair comparison."
      ],
      "metadata": {
        "id": "jJpMVpslJCw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost on balanced training set\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.07,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_weight=6,\n",
        "    gamma=2,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "xgb_results = evaluate_model(xgb, X_train_bal, y_train_bal, X_test, y_test, model_name=\"XGBoost\")\n",
        "results_list.append(xgb_results)"
      ],
      "metadata": {
        "id": "1DqW4p5-JSBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Train and Evaluate MLP Neural Network**\n",
        "\n",
        "Train a Multi-Layer Perceptron (MLP) neural network. Evaluate and visualize its classification performance."
      ],
      "metadata": {
        "id": "PAoYdgCIJCki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Train Multi-layer Perceptron on balanced training set\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.01,\n",
        "    batch_size=64,\n",
        "    learning_rate_init=0.002,\n",
        "    max_iter=200,\n",
        "    early_stopping=True,\n",
        "    random_state=42\n",
        ")\n",
        "mlp.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "mlp_results = evaluate_model(mlp, X_train_bal, y_train_bal, X_test, y_test, model_name=\"MLP Neural Network\")\n",
        "results_list.append(mlp_results)"
      ],
      "metadata": {
        "id": "kI3wGNRXJXlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Define the Transformer Model Evaluation Function**\n",
        "\n",
        "Define a specialized evaluation function for transformer-based models, capturing all metrics and artifacts needed for LLM-assisted interpretation."
      ],
      "metadata": {
        "id": "L1wlvmm0JWU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_texts, test_texts, train_labels_raw, test_labels_raw = train_test_split(\n",
        "    df[\"clinical_note\"].tolist(),\n",
        "    df[\"note_status\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Manual label mapping for strict order: 0=Stable, 1=Recovering, 2=Deteriorating, 3=Critical\n",
        "status2id = {\n",
        "    \"Stable\": 0,\n",
        "    \"Recovering\": 1,\n",
        "    \"Deteriorating\": 2,\n",
        "    \"Critical\": 3\n",
        "}\n",
        "id2status = {v: k for k, v in status2id.items()}\n",
        "\n",
        "# Map original labels to integer labels\n",
        "train_labels = [status2id[x] for x in train_labels_raw]\n",
        "test_labels  = [status2id[x] for x in test_labels_raw]\n",
        "\n",
        "print(\"Label mapping:\", status2id)\n",
        "print(\"Train label unique values:\", set(train_labels))\n",
        "print(\"Test label unique values:\", set(test_labels))\n"
      ],
      "metadata": {
        "id": "ngM6SHi3Sv_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_transformer_model(y_true, y_pred, model_name=\"Model\", target_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate a transformer model's predictions and print confusion matrix and metrics.\n",
        "    Returns summary dictionary for results table, including confusion matrix and classification report.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    prec_macro = precision_score(y_true, y_pred, average=\"macro\")\n",
        "    recall_macro = recall_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
        "    print(f\"Accuracy: {acc:.4f} | Macro F1: {f1_macro:.4f} | Precision: {prec_macro:.4f} | Recall: {recall_macro:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Classification reports\n",
        "    report_dict = classification_report(y_true, y_pred, target_names=target_names, digits=3, output_dict=True)\n",
        "    report_str = classification_report(y_true, y_pred, target_names=target_names, digits=3)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report_str)\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Test F1\": round(f1_macro, 4),\n",
        "        \"Accuracy\": round(acc, 4),\n",
        "        \"Precision\": round(prec_macro, 4),\n",
        "        \"Recall\": round(recall_macro, 4),\n",
        "        \"Confusion Matrix\": cm.tolist(),                # <-- For saving or LLM prompt\n",
        "        \"Classification Report (dict)\": report_dict,    # <-- For LLM, code, summary\n",
        "        \"Classification Report (str)\": report_str       # <-- For direct prompt/human reading\n",
        "    }"
      ],
      "metadata": {
        "id": "H8RLPKcGfY0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable external logging\n",
        "\n",
        "def train_and_predict_transformer(model_ckpt, train_texts, train_labels, test_texts, test_labels, model_name=\"Transformer\"):\n",
        "    # 1. Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=4)\n",
        "\n",
        "    # 2. Tokenize texts\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = Dataset.from_dict({**train_encodings, \"label\": train_labels})\n",
        "    test_dataset = Dataset.from_dict({**test_encodings, \"label\": test_labels})\n",
        "\n",
        "    # 3. Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # 4. Train\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # 5. Predict\n",
        "    preds = trainer.predict(test_dataset)\n",
        "    y_pred = np.argmax(preds.predictions, axis=1)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "7sVpyJwsfdp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Train and Evaluate BERT-base Transformer**\n",
        "\n",
        "Fine-tune a BERT-base transformer on clinical note text for health status prediction. Evaluate its test performance with standard metrics and confusion matrix."
      ],
      "metadata": {
        "id": "hvJk2V2yzVXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['Stable', 'Recovering', 'Deteriorating', 'Critical']\n",
        "\n",
        "# 1. BERT-base-uncased\n",
        "bert_pred = train_and_predict_transformer(\"bert-base-uncased\", train_texts, train_labels, test_texts, test_labels, model_name=\"BERT-base\")\n",
        "results_list.append(evaluate_transformer_model(test_labels, bert_pred, \"BERT-base\", target_names=target_names))"
      ],
      "metadata": {
        "id": "AwTCbVlpfmV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Train and Evaluate BioBERT Transformer**\n",
        "\n",
        "Fine-tune a BioBERT transformer model on the same prediction task. Assess its results using identical metrics for comparison."
      ],
      "metadata": {
        "id": "EjKuZRR5zdty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. BioBERT\n",
        "biobert_pred = train_and_predict_transformer(\"dmis-lab/biobert-base-cased-v1.1\", train_texts, train_labels, test_texts, test_labels, model_name=\"BioBERT\")\n",
        "results_list.append(evaluate_transformer_model(test_labels, biobert_pred, \"BioBERT\", target_names=target_names))"
      ],
      "metadata": {
        "id": "yc1fE3DOfqfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Train and Evaluate DeBERTa Transformer**\n",
        "\n",
        "Fine-tune a DeBERTa transformer model for the multi-class classification task. Evaluate and compare its predictive performance."
      ],
      "metadata": {
        "id": "x5hFcPrCztoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DeBERTa\n",
        "deberta_pred = train_and_predict_transformer(\"microsoft/deberta-base\", train_texts, train_labels, test_texts, test_labels, model_name=\"DeBERTa\")\n",
        "results_list.append(evaluate_transformer_model(test_labels, deberta_pred, \"DeBERTa\", target_names=target_names))"
      ],
      "metadata": {
        "id": "g40bwSRKfrXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Summarize and Compare All Model Results**\n",
        "\n",
        "Aggregate all results into a summary table for direct comparison across all traditional and transformer-based models."
      ],
      "metadata": {
        "id": "Ws85nJcpjR1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_df = pd.DataFrame(results_list)\n",
        "display(summary_df)"
      ],
      "metadata": {
        "id": "IqFnk2F2fxpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§‘â€ðŸ”¬7. LLM-Assisted Model Interpretation and Reporting**"
      ],
      "metadata": {
        "id": "K50WmzBQrhjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leverage a Large Language Model (LLM) such as GPT-4o to automatically interpret, compare, and summarize the predictive performance of all evaluated models. This enables objective, human-readable scientific reporting and evidence-based model selection."
      ],
      "metadata": {
        "id": "TNB23NUDrpXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Prepare the Model Performance Summary Table**\n",
        "\n",
        "Convert the pandas summary table of all model results into a markdown-formatted string for easier consumption by an LLM."
      ],
      "metadata": {
        "id": "mdCv1fqRsZG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select key columns for summary and convert to markdown for LLM input\n",
        "summary_table_text = summary_df[[\"Model\", \"Test F1\", \"Accuracy\", \"Precision\", \"Recall\"]].to_markdown(index=False)"
      ],
      "metadata": {
        "id": "Jg67tHpjsXlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define an Expert Prompt for the LLM**\n",
        "\n",
        "Write an instruction prompt that asks the LLM to analyze and summarize the model comparison table with scientific rigor and clarity."
      ],
      "metadata": {
        "id": "XHMGQ83qtWIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_SUMMARY_PROMPT = \"\"\"\n",
        "You are an expert data scientist. Below is a summary table reporting key test set performance metrics (Test F1, Accuracy, Precision, Recall) for six machine learning models (three traditional and three transformer-based) on a multi-class clinical status prediction task.\n",
        "\n",
        "Summary Table (test set results):\n",
        "\n",
        "{summary_table}\n",
        "\n",
        "Instructions:\n",
        "1. Compare the performance of traditional machine learning models (Random Forest, XGBoost, MLP Neural Network) with transformer-based models (BERT-base, BioBERT, DeBERTa), citing specific metrics and models by name.\n",
        "2. Identify the best-performing model(s) and justify your conclusion with numerical evidence.\n",
        "3. Discuss interesting trends, weaknesses, or trade-offs, such as class imbalance, computational resources, and overfitting risks.\n",
        "4. Briefly comment on each model's practicality for clinical deployment, considering real-world resource or interpretability constraints.\n",
        "5. Suggest one area for further improvement or future research.\n",
        "6. Conclude with a clear, formal academic recommendation (1-2 sentences).\n",
        "\n",
        "Write your summary in concise, formal, and academic English, suitable for a scientific report. Use bullet points if appropriate for clarity.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "WrVon501tabz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Format and Compose the Final Prompt**\n",
        "\n",
        "Insert the model performance table into the prompt template for LLM processing."
      ],
      "metadata": {
        "id": "fpzWCe9WtdGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the prompt with the actual model performance table\n",
        "final_prompt = LLM_SUMMARY_PROMPT.format(summary_table=summary_table_text)"
      ],
      "metadata": {
        "id": "_-YtCoOPtlH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Generate an Expert Summary via LLM**\n",
        "\n",
        "Send the formatted prompt to your LLM API (e.g., Azure, OpenAI GPT-4o) and print the summary for reporting."
      ],
      "metadata": {
        "id": "8etuzt9Atp01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call your LLM (replace with your actual LLM function, e.g., OpenAI/Azure call)\n",
        "llm_response = model_prompt(final_prompt, system_prompt=\"You are an expert clinical data scientist. Write in academic style.\")\n",
        "\n",
        "# Output the summary for inclusion in your report\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "RgCSrtBptnJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}