{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1AlG6-XxHVNyOP6qH56rwvGLGuAfcNkl-",
      "authorship_tag": "ABX9TyP2kgJqqmvt1jr52zkSn6Dh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorraineWong/WQD7005-Data-Mining-S2152880/blob/main/Assignment_patient_data_simulation_clear_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“¦ 0. Setup and Configuration**"
      ],
      "metadata": {
        "id": "_e3ul-5h2FDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the runtime environment for AI-assisted patient simulation and analysis by:\n",
        "\n",
        "*   Installing required dependencies\n",
        "*   Initializing credentials for Azure OpenAI and Hugging Face\n",
        "*   Validating API keys via secure secrets.json loading"
      ],
      "metadata": {
        "id": "QBOEiqHAOk3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Install Core Dependencies**\n",
        "\n",
        "Install the essential Python libraries needed for GenAI data generation and EDA."
      ],
      "metadata": {
        "id": "W4odbMzjN5mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Core Dependencies\n",
        "!pip install -q openai numpy pandas\n",
        "!pip install -q ydata-profiling\n",
        "!pip install -q transformers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "2kugXK56Ivlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Set Working Directory**\n",
        "\n",
        "All generated data files (e.g., raw outputs, datasets, summaries) will be saved to this path for clarity and version control."
      ],
      "metadata": {
        "id": "W-bg4zhK2yG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Working Directory\n",
        "my_file_path = \"/content/drive/MyDrive/UM Data Science Course Information/WQD7005/Assignment Project/\""
      ],
      "metadata": {
        "id": "_6HOZczn23Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Setup Hugging Face Token**\n",
        "\n",
        "plan to access transformer models (e.g., MiniLM, BERT-tiny), login to Hugging Face Hub is required."
      ],
      "metadata": {
        "id": "Y0u3blm_24dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Hugging Face Token\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "qOIKptvK5gEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Securely Load Azure API Credentials**\n",
        "\n",
        "Using secrets.json avoids hardcoding sensitive information. This supports secure API usage and easier sharing of my notebook."
      ],
      "metadata": {
        "id": "zPJsGReO3GeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely Load Azure API Credentials\n",
        "# Azure endpoint and keys\n",
        "import json\n",
        "\n",
        "# Load secrets.json after upload\n",
        "with open(my_file_path+\"secrets.json\", \"r\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "endpoint = secrets[\"AZURE_ENDPOINT\"]\n",
        "subscription_key = secrets[\"AZURE_KEY\"]"
      ],
      "metadata": {
        "id": "HpsM2sGkK9qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Configure Azure OpenAI Client and Define GPT Prompt Wrapper**\n",
        "\n",
        "*   **api_version**: ensures compatibility with latest GPT-4o deployment and\n",
        "*   **deployment**: name matches my Azure OpenAI Studio setting.\n",
        "*   **model_prompt**: This utility function encapsulates the entire GPT-4o interaction, allowing clean and reusable prompting for various sections (data simulation, summarization, classification, etc.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euL8-aeG4I0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Supporting Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from openai import AzureOpenAI\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Configure Azure OpenAI Client\n",
        "api_version = \"2024-12-01-preview\"\n",
        "deployment = \"gpt-4o\"\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "# Prompt execution wrapper for reuse\n",
        "def model_prompt(prompt, system_prompt=\"Act as a professional clinicians.\", temperature=0.7, max_tokens=4096):\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "LFEgr22edvyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Single Sample Data Generation via GPT (Validation Prompt)**\n",
        "\n",
        "To verify the response structure of GPT-4o by generating a realistic single-patient daily monitoring record, ensuring the output conforms to expected JSON schema for later batch generation."
      ],
      "metadata": {
        "id": "tlJZrmccPUQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Sample Data Generation via model\n",
        "data_prompt = \"\"\"\n",
        "Generate a single, realistic patient monitoring record for one randomly selected adult patient.\n",
        "\n",
        "Provide the following fields:\n",
        "- oxygen_saturation (in %)\n",
        "- heart_rate (in bpm)\n",
        "- temperature (in Â°C)\n",
        "- blood_pressure (systolic/diastolic, e.g. \"120/80\")\n",
        "- weight (in kg)\n",
        "- blood_glucose (in mg/dL)\n",
        "\n",
        "At the end, include a brief clinical_note (1â€“2 sentences, max 30 words) summarizing the patient status based on the values above. Use professional clinical tone with realistic variation (e.g. stable, recovering, mild concerns).\n",
        "\n",
        "Output as a valid JSON object with keys:\n",
        "oxygen_saturation, heart_rate, temperature, blood_pressure, weight, blood_glucose, clinical_note.\n",
        "\n",
        "Constraints:\n",
        "- Only output one JSON object.\n",
        "- No markdown or explanation.\n",
        "- Include realistic variation across different health conditions (e.g. fatigue, post-op, dietary changes, stress).\n",
        "- Ensure all fields are complete, no missing values.\n",
        "\"\"\"\n",
        "\n",
        "print(model_prompt(data_prompt))"
      ],
      "metadata": {
        "id": "zSjfEl3ZOf-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© 1. Dataset Simulation using GenAI**"
      ],
      "metadata": {
        "id": "zw4s440yQ0pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section outlines the complete process of generating synthetic inpatient monitoring data using a GenAI-powered approach.\n",
        "It simulates a realistic clinical setting where multiple patients are monitored daily for up to 30 days.\n",
        "Vital signs and clinical notes are generated based on assigned medical scenarios, ensuring medically coherent trends suitable for downstream exploratory analysis and machine learning modeling."
      ],
      "metadata": {
        "id": "waaWdJZ1P-D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1: Prompt Engineering with Clinical Context**\n",
        "\n",
        "The prompt defines the required fields (vital signs + clinical note), instructs model on how to simulate recovery trends, controls for missing value logic, and constrains the output to valid JSON without extra formatting.\n",
        "\n",
        "**2: Generate and Save Raw Outputs**\n",
        "\n",
        "To generate 30-day clinical data per patient using model and safely store each raw response as .txt for later parsing.\n",
        "\n",
        "**3: Parse and Build the Final Dataset**\n",
        "\n",
        "To transform model-generated .txt files into a clean and structured DataFrame, validating JSON format and extracting fields.\n",
        "\n",
        "**4: Final Export to CSV**\n",
        "\n",
        "To save the simulated and parsed data in a structured format suitable for downstream analytics and visualization."
      ],
      "metadata": {
        "id": "oCSmihMnd517"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Configuration and Library Setup**\n",
        "\n",
        "Import essential libraries (e.g., pandas, numpy, json, re, glob) and define basic configuration. This ensures the simulation pipeline runs in a clean and reproducible environment."
      ],
      "metadata": {
        "id": "iGBkIpf3e5Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Configuration\n",
        "num_patients = 500\n",
        "start_date_str = \"2025-01-01\"\n",
        "raw_output_dir = os.path.join(my_file_path, \"data\")\n",
        "os.makedirs(raw_output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "XL4nYVDRRC29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Prompt Engineering with Clinical Context**\n",
        "\n",
        "Design a dynamic prompt that tells model GPT-4o:\n",
        "1. To act as a clinical simulation engine\n",
        "\n",
        "2. To choose one of 4 scenarios (e.g., post-surgery, infection, chronic illness, or acute deterioration)\n",
        "\n",
        "3. To generate daily vitals and notes for each patient with 10â€“30 days of monitoring\n",
        "\n",
        "4. To follow professional tone and simulate realistic recovery or decline\n",
        "\n",
        "5. To occasionally omit 1 field (e.g., temperature, weight, blood_glucose) on 1â€“3 days\n",
        "\n",
        "This prompt ensures data is diverse, realistic, and medically grounded."
      ],
      "metadata": {
        "id": "uCj-fYYVfIU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prompt Generator 1 ===\n",
        "def generate_patient_prompt(patient_id, start_date):\n",
        "    return f\"\"\"\n",
        "You are a clinical simulation engine.\n",
        "\n",
        "Generate a valid JSON array of **daily hospital monitoring records** for patient {patient_id}, starting from {start_date} (YYYY-MM-DD).\n",
        "\n",
        "---\n",
        "\n",
        "SCENARIO & DURATION INSTRUCTIONS:\n",
        "- Randomly choose ONE of the following clinical scenarios for the patient:\n",
        "  1. **Post-surgery recovery** â†’ 18â€“30 days\n",
        "  2. **Moderate infection (e.g. pneumonia)** â†’ 15â€“25 days\n",
        "  3. **Chronic illness flare-up (e.g. diabetes complications, hypertension crisis)** â†’ 20â€“30 days\n",
        "  4. **Acute deterioration (e.g. sepsis, cardiac failure)** â†’ 10â€“15 days (with possible death or ICU transfer)\n",
        "\n",
        "- Based on the chosen scenario, simulate a realistic number of monitoring days accordingly.\n",
        "- Most patients should remain under monitoring for **more than 20 days**.\n",
        "- Only a small number of cases should stop early due to rapid recovery or deterioration.\n",
        "- If the patient recovers or deteriorates, you may stop early (but never before day 10).\n",
        "\n",
        "---\n",
        "\n",
        "EACH RECORD MUST INCLUDE:\n",
        "- date\n",
        "- oxygen_saturation (in %)\n",
        "- heart_rate (in bpm)\n",
        "- temperature (in Â°C)\n",
        "- blood_pressure (format: \"systolic/diastolic\")\n",
        "- weight (in kg)\n",
        "- blood_glucose (in mg/dL)\n",
        "- clinical_note: a brief clinical_note (2â€“3 sentences, max 35 words) summarizing the patient status based on the values above. Use professional clinical tone with realistic variation\n",
        "\n",
        "---\n",
        "\n",
        "MISSING DATA RULES:\n",
        "- On no more than 3 random days, omit 1 of: temperature, weight, blood_glucose.\n",
        "- Do not use nulls or placeholders. Simply omit the key entirely.\n",
        "\n",
        "---\n",
        "\n",
        "OUTPUT RULES:\n",
        "- Return only a **valid JSON array** of N records (N = 10 to 30 depending on scenario).\n",
        "- No markdown, No explanation, No code block.\n",
        "- Just pure JSON.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FIuJvbpbBfGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prompt Generator 2 ===\n",
        "# Monitor each patient's 30-day record of prompts\n",
        "def generate_patient_prompt_30days(patient_id, start_date):\n",
        "    return f\"\"\"\n",
        "Generate a JSON array of 30 daily monitoring records for patient {patient_id}, starting from {start_date} (YYYY-MM-DD).\n",
        "\n",
        "Each record must include:\n",
        "- date\n",
        "- oxygen_saturation (%)\n",
        "- heart_rate (bpm)\n",
        "- temperature (Â°C)\n",
        "- blood_pressure (\"systolic/diastolic\")\n",
        "- weight (kg)\n",
        "- blood_glucose (mg/dL)\n",
        "- clinical_note (2â€“3 sentences in professional tone, max 35 words)\n",
        "\n",
        "Instructions:\n",
        "- Choose a realistic medical scenario (e.g. infection, post-surgery, pregnancy, anxiety, fatigue)\n",
        "- Simulate realistic changes over 30 days: early-stage symptoms â†’ improvement â†’ stabilization or recovery\n",
        "- Use only professional clinical language, describing daily changes, recovery, and trends\n",
        "- Avoid device/system mentions\n",
        "\n",
        "Missing Data Instructions:\n",
        "- Omit at most 1 field per day, no more than 3 total days\n",
        "- Only temperature, weight, and blood_glucose may be missing\n",
        "- Do not use null/empty valuesâ€”omit keys entirely\n",
        "\n",
        "Output only a valid JSON array of 30 objects. No markdown, no code block, no explanation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1-92IKEy-1QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Generate and Save Raw Output**\n",
        "\n",
        "Loop over all patient IDs and:\n",
        "\n",
        "1. Send the prompt to GPT-4o\n",
        "2. Receive the response (a JSON array)\n",
        "3. Save the raw output to a .txt file for traceability and debugging\n",
        "\n",
        "Each patient gets one raw output file (data_output_Pxxxx.txt) stored safely."
      ],
      "metadata": {
        "id": "kjHNNjxcfoKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Phase 1: Generation and Save Raw Output ===\n",
        "for pid in range(1, num_patients + 1):\n",
        "    patient_id = f\"P{pid:04d}\"\n",
        "    prompt = generate_patient_prompt(patient_id, start_date_str)\n",
        "    raw_output = model_prompt(prompt)\n",
        "\n",
        "    with open(f\"{raw_output_dir}/data_output_{patient_id}.txt\", \"w\") as f:\n",
        "        f.write(raw_output)\n",
        "\n",
        "    print(f\"Saved output for patient id: {patient_id}.\")\n",
        "    time.sleep(0.25)"
      ],
      "metadata": {
        "id": "cL8dusxK7s_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Handling: Regenerating Patient P0264 Monitoring Records**\n",
        "\n",
        "During the JSON-to-DataFrame parsing phase, it was discovered that some patients' monitoring data had formatting issues that prevented successful conversion. This ensures the dataset remains complete and all patients are properly processed for subsequent analysis."
      ],
      "metadata": {
        "id": "zmt8zQzl8pqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Patient ID\n",
        "pid = 264\n",
        "patient_id = f\"P{pid:04d}\"\n",
        "\n",
        "# Generate Prompt\n",
        "prompt = generate_patient_prompt(patient_id, start_date_str)\n",
        "\n",
        "# Call GenAI model\n",
        "raw_output = model_prompt(prompt)\n",
        "\n",
        "# Save output\n",
        "with open(f\"{raw_output_dir}/data_output_{patient_id}.txt\", \"w\") as f:\n",
        "    f.write(raw_output)\n",
        "\n",
        "print(f\"Saved new output for patient id: {patient_id}.\")\n"
      ],
      "metadata": {
        "id": "fqrw3iGC7ufS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Parse and Build the Final Dataset**\n",
        "\n",
        "Loop over all patient IDs and:\n",
        "\n",
        "1. Validate and parse each .txt file using json.loads\n",
        "2. Extract daily records into a pandas.DataFrame\n",
        "\n",
        "This ensures that malformed or empty outputs are skipped, and clean, structured data is retained."
      ],
      "metadata": {
        "id": "oDzdfE6jf_wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Utility Functions ===\n",
        "def parse_json_from_prompt(raw_output):\n",
        "    clean_text = re.sub(r\"```json|```\", \"\", raw_output).strip()\n",
        "    return json.loads(clean_text)\n",
        "\n",
        "def is_valid_json(text):\n",
        "    try:\n",
        "        json.loads(text)\n",
        "        return True\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"JSON error:\", e)\n",
        "        return False\n",
        "# === Phase 2: Parse Raw Output into DataFrame ===\n",
        "records = []\n",
        "\n",
        "for file_path in sorted(glob.glob(f\"{raw_output_dir}/data_output_P*.txt\")):\n",
        "    patient_id = os.path.basename(file_path).split(\"_\")[-1].split(\".\")[0]\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            raw_output = f.read()\n",
        "\n",
        "        # Parse JSON\n",
        "        daily_data = json.loads(raw_output)\n",
        "        monitoring_days = len(daily_data)\n",
        "\n",
        "        for i, day in enumerate(daily_data):\n",
        "            records.append({\n",
        "                \"patient_id\": patient_id,\n",
        "                \"timestamp\": day.get(\"date\", np.nan),\n",
        "                \"oxygen_saturation\": day.get(\"oxygen_saturation\", np.nan),\n",
        "                \"heart_rate\": day.get(\"heart_rate\", np.nan),\n",
        "                \"temperature\": day.get(\"temperature\", np.nan),\n",
        "                \"blood_pressure\": day.get(\"blood_pressure\", np.nan),\n",
        "                \"weight\": day.get(\"weight\", np.nan),\n",
        "                \"blood_glucose\": day.get(\"blood_glucose\", np.nan),\n",
        "                \"clinical_note\": day.get(\"clinical_note\", \"\"),\n",
        "            })\n",
        "\n",
        "    except json.JSONDecodeError as err:\n",
        "        print(f\"JSON Decode Error for {patient_id}: {err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"Unexpected error for {patient_id}: {err}\")"
      ],
      "metadata": {
        "id": "j8le_Ome7u9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Export Final CSV**\n",
        "\n",
        "Save the final compiled dataframe into generate_patient_dataset.csv for use in later sections:\n",
        "\n",
        "1. Exploratory Data Analysis\n",
        "2. SLM-based preprocessing\n",
        "\n",
        "The exported dataset contains patient_id, timestamp, all vital signs, and clinical notes, ready for AI-driven analysis."
      ],
      "metadata": {
        "id": "8lMuhPFegc92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save parsed dataset\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(my_file_path+\"generate_patient_dataset.csv\", index=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FJ3qSbYX7w6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Š 2. Exploratory Data Analysis (EDA) Enhanced by LLMs**"
      ],
      "metadata": {
        "id": "pazycTGUEAx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section performs structured analysis on the GenAI-simulated patient dataset through both statistical exploration and large language model (LLM) interpretation. The aim is to uncover clinical trends, detect missing patterns, and summarize patient recovery trajectories across variable monitoring durations."
      ],
      "metadata": {
        "id": "Y6DAybz8McWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load and Preview the Dataset**\n",
        "\n",
        "Load the final structured dataset (generate_patient_dataset.csv) , convert the timestamp to datetime format, and perform a quick inspection of the dataset shape and features."
      ],
      "metadata": {
        "id": "34dNy26oSBr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(my_file_path + \"generate_patient_dataset.csv\")\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "\n",
        "# Preview structure\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rgFeofStFtjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Automated Full EDA with ydata-profiling**\n",
        "\n",
        "Using ydata-profiling to a comprehensive and professional-grade EDA report covering data types, statistics, missing values, correlations, interactions, distributions, and more."
      ],
      "metadata": {
        "id": "Ghxn43N2Vjtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# Generate profiling report\n",
        "profile = ProfileReport(df, title=\"GenAI Patient Dataset Exploratory Data Analysis (EDA)\", explorative=True)\n",
        "profile.to_file(my_file_path+\"eda_patient_report.html\")\n",
        "\n",
        "# Display in notebook\n",
        "with open(my_file_path+\"eda_patient_report.html\", \"r\") as f:\n",
        "    display(HTML(f.read()))"
      ],
      "metadata": {
        "id": "qe_O7OKca9-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Time Series Trend Visualization with Patient Range Control**\n",
        "\n",
        "This step visualizes temporal trends in core health indicators â€” temperature, heart rate, and blood glucose â€” for a custom-selected range of patients.\n",
        "Given the large dataset (500 patients), plotting all records together would be visually overwhelming. Therefore, I segment the visualizations by patient index range for clarity.\n",
        "\n",
        "Clinical Reference Bands:\n",
        "1. Temperature(Â°C): 36.5 ~ 37.5\n",
        "2. Heart Rate (bpm): 60 â€“ 100\n",
        "3. Blood Glucose: 70 â€“ 140 mg/dL"
      ],
      "metadata": {
        "id": "BEro1PhlXM40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Define patient range (P0100 to P0120 means index 99 to 120) ===\n",
        "start_index = 1\n",
        "end_index = 10\n",
        "\n",
        "# Get the exact patient IDs (e.g., 'P0100' to 'P0120')\n",
        "patient_ids = df['patient_id'].unique()\n",
        "selected_ids = patient_ids[start_index - 1 : end_index]\n",
        "subset_df = df[df[\"patient_id\"].isin(selected_ids)]\n",
        "\n",
        "# === Plot time series with clinical reference zones ===\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
        "\n",
        "# Temperature\n",
        "sns.lineplot(ax=axes[0], data=subset_df, x=\"timestamp\", y=\"temperature\", hue=\"patient_id\", marker=\"o\")\n",
        "axes[0].axhspan(36.5, 37.5, color='lightgreen', alpha=0.3, label='Normal Temp')\n",
        "axes[0].set_title(\"Temperature Over Time\")\n",
        "axes[0].set_ylabel(\"Â°C\")\n",
        "axes[0].legend().remove()\n",
        "\n",
        "# Heart Rate\n",
        "sns.lineplot(ax=axes[1], data=subset_df, x=\"timestamp\", y=\"heart_rate\", hue=\"patient_id\", marker=\"o\")\n",
        "axes[1].axhspan(60, 100, color='lightblue', alpha=0.3, label='Normal HR')\n",
        "axes[1].set_title(\"Heart Rate Over Time\")\n",
        "axes[1].set_ylabel(\"BPM\")\n",
        "axes[1].legend().remove()\n",
        "\n",
        "# Blood Glucose\n",
        "sns.lineplot(ax=axes[2], data=subset_df, x=\"timestamp\", y=\"blood_glucose\", hue=\"patient_id\", marker=\"o\")\n",
        "axes[2].axhspan(70, 140, color='lightcoral', alpha=0.3, label='Normal Glucose')\n",
        "axes[2].set_title(\"Blood Glucose Over Time\")\n",
        "axes[2].set_ylabel(\"mg/dL\")\n",
        "axes[2].set_xlabel(\"Date\")\n",
        "axes[2].legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=\"small\", title=\"Patient ID\")\n",
        "\n",
        "plt.suptitle(f\"Vital Sign Trends for Patients P{start_index:04d} to P{end_index:04d}\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fAr_ThS5Tjky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Patient Hospitalization Duration Distribution**\n",
        "\n",
        "This step calculates the number of days each patient remained under daily monitoring by counting the number of records per patient ID. The result is visualized as a histogram to show the distribution of hospitalization durations across all patients. This helps identify how many patients were monitored for short, typical, or full durations (up to 30 days), reflecting real-world discharge or deterioration scenarios."
      ],
      "metadata": {
        "id": "VtEWHDZPnCuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count hospitalization duration per patient\n",
        "duration_df = df.groupby(\"patient_id\").size().reset_index(name=\"days_monitored\")\n",
        "\n",
        "# Plot histogram or barplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(duration_df[\"days_monitored\"], bins=15, kde=False, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Hospitalization Duration per Patient\")\n",
        "plt.xlabel(\"Number of Days Monitored\")\n",
        "plt.ylabel(\"Number of Patients\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eZQHyoi2ncCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the top 10 patients with the longest monitoring durations\n",
        "duration_df.sort_values(\"days_monitored\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "HogI8rhVnieB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Clinical Note Summarization using LLMs (e.g., GPT-4o)**\n",
        "\n",
        "This step leverages Large Language Models (LLMs) to generate concise, high-level summaries of each patient's clinical progression across the monitoring period:\n",
        "\n",
        "1. Aggregate Clinical Notes\n",
        "Daily **clinical_note** entries are grouped by patient and concatenated into a single text block, providing a comprehensive view of the patient's condition evolution.\n",
        "\n",
        "2. Design Structured Summarization Prompt\n",
        "A structured prompt is crafted to guide the LLM in summarizing key clinical trends, such as recovery, deterioration, or stabilization, while maintaining a professional medical tone.\n",
        "\n",
        "3. LLM Invocation for Each Patient\n",
        "The structured prompt is sent to GPT-4o (or an equivalent LLM) on a per-patient basis, ensuring that each summary captures trend patterns, notable events, and the final clinical outcome.\n",
        "\n",
        "4. Export Summarized Results\n",
        "The generated summaries are consolidated into a CSV file, enabling further analysis, visualization, or integration into downstream modeling tasks."
      ],
      "metadata": {
        "id": "-i1CoZ2uXggM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group and prepare all_notes\n",
        "patient_notes = df.groupby(\"patient_id\")[\"clinical_note\"].apply(lambda x: \"\\n\".join(x)).reset_index()\n",
        "patient_notes.columns = [\"patient_id\", \"all_notes\"]\n",
        "\n",
        "# Define system prompt\n",
        "system_prompt = \"You are a senior clinical analyst. Summarize patient trend professionally.\"\n",
        "\n",
        "# Generate prompts\n",
        "def generate_summary_prompt(pid, notes):\n",
        "    return f\"\"\"\n",
        "Patient ID: {pid}\n",
        "{notes}\n",
        "Summarize in 2â€“3 sentences:\n",
        "- Overall trend\n",
        "- Turning points\n",
        "- Final outcome\n",
        "\"\"\"\n",
        "summary_prompts = patient_notes.apply(lambda row: generate_summary_prompt(row[\"patient_id\"], row[\"all_notes\"]), axis=1)"
      ],
      "metadata": {
        "id": "fgGshkG5WWF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "summaries = []\n",
        "\n",
        "for idx, prompt in tqdm(summary_prompts.items(), total=len(summary_prompts), desc=\"Generating Summaries\"):\n",
        "    pid = patient_notes.loc[idx, \"patient_id\"]\n",
        "    output = model_prompt(prompt, system_prompt=system_prompt)\n",
        "    summaries.append({\n",
        "        \"patient_id\": pid,\n",
        "        \"summary\": output\n",
        "    })"
      ],
      "metadata": {
        "id": "f_O01XX9a9IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the summary dataset\n",
        "summary_df = pd.DataFrame(summaries)\n",
        "summary_df.to_csv(my_file_path+\"patient_summary.csv\", index=False)\n",
        "summary_df.head()"
      ],
      "metadata": {
        "id": "pnYGUjJCl1Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§¼ 3. Advanced Data Preprocessing with SLMs / LLMs**"
      ],
      "metadata": {
        "id": "HfS-wegWETmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section demonstrates the use of advanced preprocessing techniques to prepare a patient monitoring dataset for analysis or modeling. The process includes intelligent missing value handling, feature normalization, categorical feature encoding, application of Small Language Models (SLMs) for text embedding or enhancement"
      ],
      "metadata": {
        "id": "p7si5MGHnmgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Categorical Feature Encoding**\n",
        "\n",
        "The composite blood_pressure string (e.g., \"120/80\") is split into two separate numerical columns: systolic_bp and diastolic_bp. This improves model interpretability and allows for numerical analysis or binning of these values."
      ],
      "metadata": {
        "id": "1yO3_LbUw6TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.copy()\n",
        "df_cleaned[[\"systolic_bp\", \"diastolic_bp\"]] = df[\"blood_pressure\"].str.split(\"/\", expand=True).astype(float)\n",
        "df_cleaned = df_cleaned.drop(columns=[\"blood_pressure\"])"
      ],
      "metadata": {
        "id": "cvEEhwZsxFz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview\n",
        "df_cleaned.info()\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "Q1j1CEGi8BGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Intelligent Per-Patient Missing Value Handling**\n",
        "\n",
        "To preserve individual variation in clinical patterns, missing values in key physiological metrics â€” including temperature, weight, blood glucose, and blood pressure (systolic and diastolic) â€” are imputed separately for each patient using their own median values. This patient-centric approach ensures that imputation aligns with personal baselines and avoids distortion from population-level statistics, maintaining the integrity of temporal health trends for downstream analysis."
      ],
      "metadata": {
        "id": "49_5PRRCoH2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fill_cols = [\"temperature\", \"weight\", \"blood_glucose\", \"systolic_bp\", \"diastolic_bp\"]\n",
        "for col in fill_cols:\n",
        "    df_cleaned[col] = df_cleaned.groupby(\"patient_id\")[col].transform(lambda x: x.fillna(x.median()))"
      ],
      "metadata": {
        "id": "6I7CY8oEFt3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview\n",
        "df_cleaned.info()\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "a_2oaVVH9e1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Numerical Feature Normalization**\n",
        "\n",
        "To ensure fair comparisons between patients and to prevent scale dominance during model training, we applied Z-score normalization to key numerical features. This transformation centers values around zero and scales them based on standard deviation, enabling balanced feature contributions across machine learning models such as Neural Networks and gradient-boosted trees. The normalized features include:\n",
        "\n",
        "*   temperature\n",
        "*   heart_rate\n",
        "*   blood_glucose\n",
        "*   oxygen_saturation\n",
        "*   systolic_bp\n",
        "*   diastolic_bp\n",
        "*   weight\n",
        "\n",
        "Each original column is transformed into a new column with _zscore suffix (e.g., temperature_zscore), while the original columns will be dropped to reduce redundancy and improve clarity in downstream modeling."
      ],
      "metadata": {
        "id": "xaqV9HgFwo49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "norm_cols = [\"temperature\", \"heart_rate\", \"blood_glucose\", \"oxygen_saturation\", \"systolic_bp\", \"diastolic_bp\", \"weight\"]\n",
        "scaler = StandardScaler()\n",
        "df_cleaned[[f\"{col}_zscore\" for col in norm_cols]] = scaler.fit_transform(df_cleaned[norm_cols])\n",
        "df_cleaned.drop(columns=norm_cols, inplace=True)"
      ],
      "metadata": {
        "id": "dN6XbD_twod3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview\n",
        "df_cleaned.info()\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "wcLsiUgp9aeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: LLM-Based Clinical Note Classification using Hugging Face**\n",
        "\n",
        "To automate the labeling of patient clinical status, we applied a zero-shot classification pipeline using the `facebook/bart-large-mnli` model via Hugging Face Transformers. Each clinical note is classified into one of the predefined categories: **Stable**, **Recovering**, **Deteriorating**, or **Critical**.\n",
        "\n",
        "This approach leverages a language modelâ€™s semantic understanding to map raw free-text medical notes into structured health status labels, without requiring manual annotation or rule-based logic.\n",
        "\n",
        "The output (`note_status`) is then label-encoded into `note_status_encoded` for model training.\n",
        "\n",
        "Label Definitions:\n",
        "*   Stable â€“ Patient remained within normal clinical limits.\n",
        "*   Recovering â€“ Signs of gradual improvement observed.\n",
        "*   Deteriorating â€“ Abnormality or worsening symptoms detected.\n",
        "*   Critical â€“ Urgent care required due to severe condition.\n",
        "\n",
        "This method supports downstream supervised model training, enabling the development of predictive models based on both structured data and LLM-generated labels."
      ],
      "metadata": {
        "id": "In1mD3lcxYPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Device Detection (GPU if available)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"GPU\" if device == 0 else \"CPU\")"
      ],
      "metadata": {
        "id": "rjtINCOMawM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load zero-shot classifier\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=device\n",
        ")\n",
        "# Define medical status labels\n",
        "labels = [\"Stable\", \"Recovering\", \"Deteriorating\", \"Critical\"]\n",
        "\n",
        "# Initialize empty column for note_status\n",
        "df_cleaned[\"note_status\"] = None"
      ],
      "metadata": {
        "id": "xVrD_GNvoxSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Start classification\n",
        "print(\"Starting clinical note classification with Hugging Face zero-shot model...\\n\")\n",
        "\n",
        "for idx, text in tqdm(enumerate(df_cleaned[\"clinical_note\"]), total=len(df_cleaned), desc=\"Classifying Notes\"):\n",
        "    result = classifier(text, candidate_labels=labels)\n",
        "    df_cleaned.loc[idx, \"note_status\"] = result[\"labels\"][0]  # Choose the most likely label"
      ],
      "metadata": {
        "id": "BoW3mIN8o-Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview sample\n",
        "df_cleaned[[\"patient_id\", \"timestamp\", \"clinical_note\", \"note_status\"]].head()"
      ],
      "metadata": {
        "id": "CHeO3lxdpvqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of classified note_status labels.\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(\n",
        "    data=df_cleaned,\n",
        "    x=\"note_status\",\n",
        "    palette=\"pastel\",\n",
        "    order=df_cleaned[\"note_status\"].value_counts().index\n",
        ")\n",
        "plt.title(\"Distribution of Clinical Note Status Labels\")\n",
        "plt.xlabel(\"Note Status\")\n",
        "plt.ylabel(\"Number of Records\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_cleaned[\"note_status\"].value_counts()"
      ],
      "metadata": {
        "id": "_eBz-ek6OApl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_cleaned[\"note_status_encoded\"] = label_encoder.fit_transform(df_cleaned[\"note_status\"])\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "label_mapping"
      ],
      "metadata": {
        "id": "wjWRrw56-mdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview\n",
        "df_cleaned.info()\n",
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "a3_sp53j_U9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save classified results\n",
        "df_cleaned.to_csv(my_file_path + \"preprocessing_generate_patient_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "QN_QSnLY-iHz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
